{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONSTRUCT CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def corpus_creator(file_paths):\n",
    "    corpus = []\n",
    "    for file_path in file_paths:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "        # Iterate over rows and add each row to the corpus\n",
    "        for index, row in df.iterrows():\n",
    "            # Assuming each row is a document\n",
    "            document = ' '.join(row.astype(str))\n",
    "            corpus.append(document)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths:\n",
    "csv_files = [\n",
    "    'path/to/first_file.csv',\n",
    "    'path/to/second_file.csv',\n",
    "    # Add more file paths here\n",
    "]\n",
    "\n",
    "# Create the corpus from the CSV files\n",
    "corpus = corpus_creator(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a set of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Function to perform lemmatization\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Function to remove rare words\n",
    "def remove_rare_words(tokens, min_count=2):\n",
    "    token_counts = Counter(tokens)\n",
    "    return [token for token in tokens if token_counts[token] >= min_count]\n",
    "\n",
    "# Preprocess the corpus\n",
    "def preprocess_corpus(corpus):\n",
    "    processed_corpus = []\n",
    "    for doc in corpus:\n",
    "        text = remove_punctuation(doc)\n",
    "        tokens = tokenize(text)\n",
    "        tokens = remove_stop_words(tokens)\n",
    "        tokens = lemmatize_tokens(tokens)\n",
    "        tokens = remove_rare_words(tokens)\n",
    "        processed_corpus.append(' '.join(tokens))\n",
    "    return processed_corpus\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the corpus\n",
    "corpus = preprocess_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VECTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA IMPLIMENTATION WITH GIBBS-SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lda(X, K, alpha, beta, iterations):\n",
    "    D, V = X.shape\n",
    "    topic_assignments = np.random.randint(K, size=(D, V))\n",
    "    doc_topic_counts = np.zeros((D, K)) + alpha\n",
    "    topic_word_counts = np.zeros((K, V)) + beta\n",
    "    topic_counts = np.zeros(K) + V * beta\n",
    "\n",
    "    for d in range(D):\n",
    "        for v in range(V):\n",
    "            word_count = X[d, v]\n",
    "            topic = topic_assignments[d, v]\n",
    "            doc_topic_counts[d, topic] += word_count\n",
    "            topic_word_counts[topic, v] += word_count\n",
    "            topic_counts[topic] += word_count\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        for d in range(D):\n",
    "            for v in range(V):\n",
    "                word_count = X[d, v]\n",
    "                if word_count == 0:\n",
    "                    continue\n",
    "                old_topic = topic_assignments[d, v]\n",
    "                doc_topic_counts[d, old_topic] -= word_count\n",
    "                topic_word_counts[old_topic, v] -= word_count\n",
    "                topic_counts[old_topic] -= word_count\n",
    "                topic_probs = (doc_topic_counts[d, :] *\n",
    "                               topic_word_counts[:, v] /\n",
    "                               topic_counts)\n",
    "                topic_probs /= topic_probs.sum()\n",
    "                new_topic = np.random.choice(K, p=topic_probs)\n",
    "                doc_topic_counts[d, new_topic] += word_count\n",
    "                topic_word_counts[new_topic, v] += word_count\n",
    "                topic_counts[new_topic] += word_count\n",
    "                topic_assignments[d, v] = new_topic\n",
    "\n",
    "    phi = topic_word_counts / topic_counts[:, np.newaxis]\n",
    "    theta = doc_topic_counts / doc_topic_counts.sum(axis=1)[:, np.newaxis]\n",
    "    return phi, theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for hyperparameter tuning\n",
    "def grid_search_lda(X, K_values, alpha_values, beta_values, iterations):\n",
    "    best_K = K_values[0]\n",
    "    best_alpha = alpha_values[0]\n",
    "    best_beta = beta_values[0]\n",
    "    best_score = float('inf')\n",
    "\n",
    "    for K in K_values:\n",
    "        for alpha in alpha_values:\n",
    "            for beta in beta_values:\n",
    "                phi, theta = run_lda(X, K, alpha, beta, iterations)\n",
    "                score = calculate_score(phi, theta)  # Define your scoring function\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_K = K\n",
    "                    best_alpha = alpha\n",
    "                    best_beta = beta\n",
    "\n",
    "    return best_K, best_alpha, best_beta\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "K_values = [5, 10, 15]\n",
    "alpha_values = [0.1, 0.5, 1.0]\n",
    "beta_values = [0.01, 0.05, 0.1]\n",
    "\n",
    "# Perform grid search\n",
    "best_K, best_alpha, best_beta = grid_search_lda(X, K_values, alpha_values, beta_values, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN WITH BEST PARAMETERS AND PRINT RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA with the best hyperparameters\n",
    "phi, theta = run_lda(X, best_K, best_alpha, best_beta, 1000)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Best K: {best_K}, Best Alpha: {best_alpha}, Best Beta: {best_beta}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
